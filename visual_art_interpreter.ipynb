{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visual Art Interpreter - Phase 1: Zero-Shot Baseline\n",
        "\n",
        "Model: Qwen3-VL-8B-Instruct\n",
        "Task: Zero-shot art/aesthetic analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate bitsandbytes datasets\n",
        "!pip install -q pillow matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model (Qwen3-VL-8B-Instruct)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
        "\n",
        "# Load in bfloat16 (A100 has enough VRAM; use load_in_4bit=True for smaller GPUs)\n",
        "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")\n",
        "print(f\"Device: {model.device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Output directory (local - no Google Drive)\n",
        "OUTPUT_DIR = Path(\"./outputs\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load AVA dataset from Hugging Face (small batch for testing)\n",
        "# Dataset: image_id, image, mean_score (1-10), rating_counts, total_votes, etc.\n",
        "# NOTE: split=\"train[:20]\" downloads ALL 61 shards (~30GB). Load parquet directly\n",
        "# to fetch only the first shard (~500MB) and bypass split metadata.\n",
        "BATCH_SIZE = 20  # Adjust for quick testing\n",
        "\n",
        "# Load single parquet file via generic loader (avoids ExpectedMoreSplitsError)\n",
        "AVA_PARQUET_URL = \"https://huggingface.co/datasets/trojblue/AVA-Huggingface/resolve/main/data/train-00000-of-00061.parquet\"\n",
        "ava_dataset = load_dataset(\"parquet\", data_files=AVA_PARQUET_URL, split=\"train\")\n",
        "ava_dataset = ava_dataset.select(range(BATCH_SIZE))\n",
        "\n",
        "print(f\"Loaded {len(ava_dataset)} images from AVA-Huggingface (~500MB download)\")\n",
        "print(f\"Columns: {ava_dataset.column_names}\")\n",
        "print(f\"Sample mean_score range: {ava_dataset['mean_score'][0]:.2f} - {ava_dataset['mean_score'][-1]:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert AVA dataset to list of dicts for evaluation\n",
        "def prepare_ava_batch(dataset):\n",
        "    \"\"\"Convert AVA dataset to list of {image_id, image, mean_score}.\"\"\"\n",
        "    return [\n",
        "        {\n",
        "            \"image_id\": row[\"image_id\"],\n",
        "            \"image\": row[\"image\"].convert(\"RGB\") if hasattr(row[\"image\"], \"convert\") else row[\"image\"],\n",
        "            \"mean_score\": float(row[\"mean_score\"]),\n",
        "        }\n",
        "        for row in dataset\n",
        "    ]\n",
        "\n",
        "test_images = prepare_ava_batch(ava_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Prepared {len(test_images)} images for evaluation\")\n",
        "print(f\"Sample: image_id={test_images[0]['image_id']}, mean_score={test_images[0]['mean_score']:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation Prompts (8-Question Framework)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prompts based on Pad√≥ & Thomas (2025)\n",
        "EVAL_PROMPTS = {\n",
        "    \"aesthetic\": \"Rate this photograph's aesthetic quality from 1-10 (1=low, 10=high). Give your score as a single number first, then briefly explain.\",\n",
        "    \"content\": \"Describe what you see in this artwork.\",\n",
        "    \"type\": \"What type of artwork is this (painting, photograph, etc.)?\",\n",
        "    \"emotion\": \"What emotion does this artwork convey?\",\n",
        "    \"polarity\": \"Is the overall emotion positive or negative?\",\n",
        "    \"specific_emotion\": \"What specific emotion is depicted (joy, grief, awe, etc.)?\",\n",
        "    \"expression\": \"How is this emotion expressed (color, composition, subject)?\",\n",
        "    \"symbol\": \"Are there any symbols used to convey meaning?\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inference Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def analyze_image(image, prompt_key=\"content\", image_id=None):\n",
        "    \"\"\"\n",
        "    Run zero-shot inference on an image.\n",
        "    \n",
        "    Args:\n",
        "        image: Path to image file (str) or PIL Image\n",
        "        prompt_key: Key from EVAL_PROMPTS dict\n",
        "        image_id: Optional identifier (for results)\n",
        "    \n",
        "    Returns:\n",
        "        dict with image info, prompt, and response\n",
        "    \"\"\"\n",
        "    # Load image if path given\n",
        "    if isinstance(image, (str, Path)):\n",
        "        image = Image.open(image).convert(\"RGB\")\n",
        "    elif hasattr(image, \"convert\"):\n",
        "        image = image.convert(\"RGB\")\n",
        "    \n",
        "    # Get prompt\n",
        "    prompt = EVAL_PROMPTS.get(prompt_key, prompt_key)\n",
        "    \n",
        "    # Prepare message (Qwen3-VL format)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert art historian and critic.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Process with Qwen3-VL API\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "\n",
        "    # Move inputs to model device (handles nested pixel_values)\n",
        "    def to_device(obj, device):\n",
        "        if hasattr(obj, \"to\"):\n",
        "            return obj.to(device)\n",
        "        if isinstance(obj, (list, tuple)):\n",
        "            return type(obj)(to_device(x, device) for x in obj)\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: to_device(v, device) for k, v in obj.items()}\n",
        "        return obj\n",
        "    inputs = to_device(inputs, model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
        "    \n",
        "    # Decode only new tokens (trim input from output)\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    generated_ids_trimmed = generated_ids[:, input_length:]\n",
        "    response = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "    \n",
        "    return {\n",
        "        \"image_id\": image_id,\n",
        "        \"prompt_key\": prompt_key,\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test on first image\n",
        "if test_images:\n",
        "    test_img = test_images[0]\n",
        "    print(f\"Testing on: image_id={test_img['image_id']} (mean_score={test_img['mean_score']:.2f})\")\n",
        "    \n",
        "    result = analyze_image(test_img[\"image\"], prompt_key=\"aesthetic\", image_id=test_img[\"image_id\"])\n",
        "    result[\"mean_score\"] = test_img[\"mean_score\"]  # Ground truth for evaluation\n",
        "    print(f\"\\nPrompt: {result['prompt']}\")\n",
        "    print(f\"\\nResponse: {result['response']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Batch Evaluation (Small Scale)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run on subset for quick validation\n",
        "def run_subset_evaluation(images, max_images=10, prompt_key=\"content\"):\n",
        "    \"\"\"Run evaluation on subset of images (from AVA or similar).\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for img_info in images[:max_images]:\n",
        "        print(f\"Processing: {img_info['image_id']}...\", end=\" \")\n",
        "        try:\n",
        "            result = analyze_image(img_info[\"image\"], prompt_key, image_id=img_info[\"image_id\"])\n",
        "            result[\"mean_score\"] = img_info[\"mean_score\"]  # Ground truth for correlation eval\n",
        "            results.append(result)\n",
        "            print(\"OK\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run on 5 images as smoke test (aesthetic prompt for AVA scoring)\n",
        "results = run_subset_evaluation(test_images, max_images=5, prompt_key=\"aesthetic\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save results\n",
        "output_file = OUTPUT_DIR / \"phase1_pilot_results.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to: {output_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}