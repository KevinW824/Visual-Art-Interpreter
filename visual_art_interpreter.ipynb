{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Art Interpreter - Phase 1: Zero-Shot Baseline\n",
    "\n",
    "Model: Qwen3-VL-8B-Instruct\n",
    "Task: Zero-shot art/aesthetic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q pillow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model (Qwen3-VL-8B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
    "\n",
    "# Load in 4-bit to save VRAM\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for data storage)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths - modify these\n",
    "DATA_DIR = Path(\"/content/drive/MyDrive/art_data\")  # Update with your path\n",
    "OUTPUT_DIR = Path(\"/content/drive/MyDrive/outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple image loader\n",
    "def load_test_images(data_dir):\n",
    "    \"\"\"Load images from directory.\n",
    "    \n",
    "    Expected structure:\n",
    "        data_dir/\n",
    "            photos/\n",
    "            paintings/\n",
    "            abstract/\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    categories = [\"photos\", \"paintings\", \"abstract\"]\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_dir = data_dir / category\n",
    "        if not cat_dir.exists():\n",
    "            continue\n",
    "        for img_path in cat_dir.glob(\"*.jpg\"):\n",
    "            images.append({\n",
    "                \"path\": str(img_path),\n",
    "                \"category\": category,\n",
    "                \"name\": img_path.stem\n",
    "            })\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images\n",
    "test_images = load_test_images(DATA_DIR)\n",
    "print(f\"Loaded {len(test_images)} images\")\n",
    "print(f\"Categories: {set(img['category'] for img in test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Prompts (8-Question Framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts based on Pad\u00f3 & Thomas (2025)\n",
    "EVAL_PROMPTS = {\n",
    "    \"content\": \"Describe what you see in this artwork.\",\n",
    "    \"type\": \"What type of artwork is this (painting, photograph, etc.)?\",\n",
    "    \"emotion\": \"What emotion does this artwork convey?\",\n",
    "    \"polarity\": \"Is the overall emotion positive or negative?\",\n",
    "    \"specific_emotion\": \"What specific emotion is depicted (joy, grief, awe, etc.)?\",\n",
    "    \"expression\": \"How is this emotion expressed (color, composition, subject)?\",\n",
    "    \"symbol\": \"Are there any symbols used to convey meaning?\",\n",
    "    \"aesthetic\": \"Rate this artwork aesthetically from 1-10 and explain why.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_path, prompt_key=\"content\"):\n",
    "    \"\"\"\n",
    "    Run zero-shot inference on an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        prompt_key: Key from EVAL_PROMPTS dict\n",
    "    \n",
    "    Returns:\n",
    "        dict with image info, prompt, and response\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get prompt\n",
    "    prompt = EVAL_PROMPTS.get(prompt_key, prompt_key)\n",
    "    \n",
    "    # Prepare message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert art historian and critic.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return {\n",
    "        \"image_path\": str(image_path),\n",
    "        \"prompt_key\": prompt_key,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first image\n",
    "if test_images:\n",
    "    test_img = test_images[0]\n",
    "    print(f\"Testing on: {test_img['name']} ({test_img['category']})\")\n",
    "    \n",
    "    result = analyze_image(test_img['path'], prompt_key=\"content\")\n",
    "    print(f\"\\nPrompt: {result['prompt']}\")\n",
    "    print(f\"\\nResponse: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Evaluation (Small Scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on subset for quick validation\n",
    "def run_subset_evaluation(images, max_images=10, prompt_key=\"content\"):\n",
    "    \"\"\"Run evaluation on subset of images.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for img_info in images[:max_images]:\n",
    "        print(f\"Processing: {img_info['name']}...\", end=\" \")\n",
    "        try:\n",
    "            result = analyze_image(img_info['path'], prompt_key)\n",
    "            result['category'] = img_info['category']\n",
    "            results.append(result)\n",
    "            print(\"OK\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on 5 images as smoke test\n",
    "results = run_subset_evaluation(test_images, max_images=5, prompt_key=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_file = OUTPUT_DIR / \"phase1_pilot_results.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Validate model works on your test images\n",
    "2. Add more prompts - test all 8 questions\n",
    "3. Expand test set - run on full evaluation dataset\n",
    "4. Add metrics - consistency checks, comparison with ground truth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}